# PostgreSQL Deep Dive

Comprehensive technical guide to PostgreSQL, an advanced open-source relational database management system. This document provides detailed technical information, high availability architectures, configuration examples, operational procedures, and troubleshooting guides for PostgreSQL administrators and engineers.

> **For deployment strategies, decision frameworks, and high-level overviews, see the [PostgreSQL Documentation](https://www.postgresql.org/docs/) and related resources.**

## Table of Contents

### Getting Started
- [Overview](#overview)
- [Architecture](#architecture)
- [Installation & Configuration](#installation--configuration)

### Data Management
- [Data Types](#data-types)
  - [Numeric Types](#numeric-types)
  - [Character Types](#character-types)
  - [Date/Time Types](#datetime-types)
  - [Boolean Type](#boolean-type)
  - [JSON and JSONB](#json-and-jsonb)
  - [Arrays](#arrays)
  - [UUID](#uuid)
  - [Custom Types](#custom-types)
  - [hstore (Key-Value Store)](#hstore-key-value-store)
  - [XML Type](#xml-type)
  - [BYTEA (Binary Data)](#bytea-binary-data)
- [Sequences and Identity Columns](#sequences-and-identity-columns)
- [Schema Design](#schema-design)
  - [DDL Commands](#ddl-commands-data-definition-language)
  - [Tables and Constraints](#tables-and-constraints)
  - [Indexes](#indexes)
  - [Foreign Keys](#foreign-keys)
  - [Schemas](#schemas)

### SQL Fundamentals
- [Introduction to SQL](#introduction-to-sql)
- [Querying Data](#querying-data)
  - [SELECT Statement](#select-statement)
  - [SELECT DISTINCT](#select-distinct)
  - [Column Aliases](#column-aliases)
  - [Table Aliases](#table-aliases)
  - [WHERE Clause](#where-clause)
  - [ORDER BY](#order-by)
  - [LIMIT and OFFSET](#limit-and-offset)
  - [SELECT INTO](#select-into)
- [SQL Operators](#sql-operators)
  - [Comparison Operators](#comparison-operators)
  - [Logical Operators](#logical-operators)
  - [Pattern Matching](#pattern-matching)
- [DML Commands](#dml-commands-data-manipulation-language)
  - [INSERT Statement](#insert-statement)
  - [UPDATE Statement](#update-statement)
  - [UPDATE with JOIN](#update-with-join)
  - [DELETE Statement](#delete-statement)
  - [UPSERT (INSERT ... ON CONFLICT)](#upsert-insert--on-conflict)
- [Joins](#joins)
  - [INNER JOIN](#inner-join)
  - [LEFT JOIN](#left-join)
  - [RIGHT JOIN](#right-join)
  - [FULL OUTER JOIN](#full-outer-join)
  - [CROSS JOIN](#cross-join)
  - [Self-Join](#self-join)
  - [Natural Join](#natural-join)
- [Aggregations and Grouping](#aggregations-and-grouping)
  - [Grouping Sets](#grouping-sets)
  - [CUBE](#cube)
  - [ROLLUP](#rollup)
  - [Set Operations](#set-operations)
- [Window Functions](#window-functions)
- [Common Table Expressions (CTEs)](#common-table-expressions-ctes)
- [Subqueries](#subqueries)
  - [ANY and ALL Operators](#any-and-all-operators)
- [Views](#views)
- [Conditional Expressions & Operators](#conditional-expressions--operators)
  - [CASE Expression](#case-expression)
  - [COALESCE](#coalesce)
  - [NULLIF](#nullif)
  - [CAST](#cast)

### Transactions & Concurrency
- [Transactions and ACID Properties](#transactions-and-acid-properties)
- [Transaction Control](#transaction-control)
- [Isolation Levels](#isolation-levels)
- [MVCC (Multi-Version Concurrency Control)](#mvcc-multi-version-concurrency-control)
- [Locking](#locking)
- [Deadlocks](#deadlocks)

### High Availability
- [High Availability Overview](#high-availability-overview)
- [HA Architectures](#ha-architectures)
  - [Patroni-based HA](#patroni-based-ha)
  - [pg_auto_failover](#pg_auto_failover)
  - [Stateful MIGs with Regional Persistent Disk](#stateful-migs-with-regional-persistent-disk)
- [Replication](#replication)
  - [Streaming Replication](#streaming-replication)
  - [Logical Replication](#logical-replication)
  - [Synchronous vs Asynchronous](#synchronous-vs-asynchronous)
- [Failover & Recovery](#failover--recovery)

### Performance & Operations
- [Performance Optimization](#performance-optimization)
  - [Query Optimization](#query-optimization)
  - [Index Optimization](#index-optimization)
  - [EXPLAIN and Query Plans](#explain-and-query-plans)
  - [pg_stat_statements](#pg_stat_statements)
- [Backup & Recovery](#backup--recovery)
  - [pg_dump and pg_restore](#pg_dump-and-pg_restore)
  - [pg_basebackup](#pg_basebackup)
  - [Continuous Archiving](#continuous-archiving)
  - [Point-in-Time Recovery (PITR)](#point-in-time-recovery-pitr)
- [Monitoring](#monitoring)
  - [System Catalogs](#system-catalogs)
  - [Statistics Views](#statistics-views)
  - [Logging](#logging)
- [Import & Export Data](#import--export-data)
  - [COPY Command](#copy-command)
- [psql Commands](#psql-commands)
- [PostgreSQL Recipes](#postgresql-recipes)
  - [Compare Two Tables](#compare-two-tables)
  - [Delete Duplicate Rows](#delete-duplicate-rows)
  - [Generate Random Number in Range](#generate-random-number-in-range)
  - [Temporary Tables](#temporary-tables)
  - [Copy Table](#copy-table)
- [Connection Pooling](#connection-pooling)
- [Vacuum and Maintenance](#vacuum-and-maintenance)

### Advanced Topics
- [Stored Procedures and Functions](#stored-procedures-and-functions)
  - [PL/pgSQL](#plpgsql)
  - [User-Defined Functions](#user-defined-functions)
- [Triggers](#triggers)
- [Extensions](#extensions)
- [Full-Text Search](#full-text-search)
- [Partitioning](#partitioning)

### Infrastructure as Code (IaC)
- [Pulumi PostgreSQL Provider](#pulumi-postgresql-provider)
  - [Installation](#pulumi-installation)
  - [Provider Configuration](#pulumi-provider-configuration)
  - [Managing Resources](#pulumi-managing-resources)
  - [Authentication Methods](#pulumi-authentication)
  - [Multi-Server Configuration](#pulumi-multi-server)

### Security
- [User and Role Management](#user-and-role-management)
- [Privileges and Permissions](#privileges-and-permissions)
- [Row-Level Security](#row-level-security)
- [Encryption](#encryption)
- [SSL/TLS Configuration](#ssltls-configuration)

### Best Practices
- [Best Practices](#best-practices)
- [Resources](#resources)

---

## Overview

PostgreSQL is a powerful, open-source object-relational database system with over 35 years of active development. It has earned a strong reputation for reliability, feature robustness, and performance. PostgreSQL runs on all major operating systems and has been ACID-compliant since 2001.

### Key Features

- **ACID Compliance**: Full ACID transaction support
- **Advanced Data Types**: JSON, JSONB, arrays, hstore, and custom types
- **Extensibility**: Extensible with custom functions, operators, and data types
- **Replication**: Built-in streaming replication and logical replication
- **High Availability**: Multiple HA solutions including Patroni, pg_auto_failover
- **Performance**: Advanced query optimizer, parallel query execution
- **Security**: Row-level security, encryption, comprehensive access control

## Architecture

PostgreSQL uses a process-per-connection model where each client connection gets its own backend process. The architecture includes:

- **Postmaster Process**: Main process that manages connections
- **Backend Processes**: One per client connection
- **Background Processes**: Autovacuum, WAL writer, Checkpointer, etc.
- **Shared Memory**: Shared buffers, WAL buffers, lock tables

## Installation & Configuration

### Installation

```bash
# Ubuntu/Debian
sudo apt update
sudo apt install postgresql postgresql-contrib

# CentOS/RHEL
sudo yum install postgresql-server postgresql-contrib
sudo postgresql-setup initdb

# macOS
brew install postgresql

# Docker
docker run --name postgres -e POSTGRES_PASSWORD=password -d postgres:15
```

### Basic Configuration

```bash
# Start service
sudo systemctl start postgresql
sudo systemctl enable postgresql

# Connect
sudo -u postgres psql
```

### Configuration File (postgresql.conf)

```ini
# Connection settings
listen_addresses = '*'
port = 5432
max_connections = 100

# Memory settings
shared_buffers = 256MB          # 25% of RAM
effective_cache_size = 1GB      # 50-75% of RAM
work_mem = 4MB                  # per operation
maintenance_work_mem = 64MB

# WAL settings
wal_level = replica              # or 'logical' for logical replication
max_wal_size = 1GB
min_wal_size = 80MB

# Checkpoint settings
checkpoint_timeout = 5min
checkpoint_completion_target = 0.9

# Logging
logging_collector = on
log_directory = 'log'
log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log'
log_statement = 'all'            # or 'ddl', 'mod', 'none'
log_duration = on
log_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h '
```

## Data Types

PostgreSQL provides a rich set of native data types. According to the [PostgreSQL documentation](https://www.postgresql.org/docs/current/datatype.html), users can also define custom data types to suit specific application needs.

### Numeric Types

PostgreSQL supports various numeric types for different use cases:

**Integer Types:**
- **SMALLINT**: 2 bytes, range -32,768 to 32,767
- **INTEGER** or **INT**: 4 bytes, range -2,147,483,648 to 2,147,483,647
- **BIGINT**: 8 bytes, range -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807
- **SERIAL**: Auto-incrementing integer (1 to 2,147,483,647)
- **BIGSERIAL**: Auto-incrementing bigint

**Decimal Types:**
- **DECIMAL** or **NUMERIC**: Variable precision, exact numeric
  ```sql
  DECIMAL(10, 2)  -- 10 digits total, 2 after decimal point
  ```
- **REAL**: 4 bytes, single precision floating point
- **DOUBLE PRECISION**: 8 bytes, double precision floating point

**Example:**
```sql
CREATE TABLE products (
    id SERIAL PRIMARY KEY,
    price DECIMAL(10, 2),
    quantity INTEGER,
    weight REAL
);
```

### Character Types

- **CHAR(n)**: Fixed-length character string, blank-padded
- **VARCHAR(n)**: Variable-length character string with limit
- **TEXT**: Variable unlimited length (no length limit)

**Example:**
```sql
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    username VARCHAR(50) UNIQUE,
    email VARCHAR(255),
    bio TEXT
);
```

### Date/Time Types

- **DATE**: Date only (YYYY-MM-DD)
- **TIME**: Time of day (HH:MM:SS)
- **TIMESTAMP**: Date and time (YYYY-MM-DD HH:MM:SS)
- **TIMESTAMPTZ**: Timestamp with time zone
- **INTERVAL**: Time span

**Example:**
```sql
CREATE TABLE events (
    id SERIAL PRIMARY KEY,
    event_date DATE,
    start_time TIMESTAMP,
    duration INTERVAL
);

INSERT INTO events (event_date, start_time, duration)
VALUES ('2024-01-15', '2024-01-15 10:00:00', '2 hours');
```

### Boolean Type

- **BOOLEAN** or **BOOL**: True, False, or NULL

**Example:**
```sql
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    username VARCHAR(50),
    is_active BOOLEAN DEFAULT TRUE
);
```

### JSON and JSONB

PostgreSQL provides native support for JSON data:

- **JSON**: Stores JSON data as text (exact copy)
- **JSONB**: Stores JSON data in binary format (more efficient, supports indexing)

**JSONB Advantages:**
- Faster to process (no parsing needed)
- Supports indexing (GIN indexes)
- Removes duplicate keys and whitespace
- Preserves key order

**Example:**
```sql
CREATE TABLE products (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100),
    attributes JSONB
);

INSERT INTO products (name, attributes)
VALUES ('Laptop', '{"brand": "Dell", "ram": "16GB", "storage": "512GB"}');

-- Query JSONB
SELECT name, attributes->>'brand' AS brand
FROM products
WHERE attributes @> '{"ram": "16GB"}';

-- Create GIN index on JSONB
CREATE INDEX idx_attributes ON products USING GIN (attributes);
```

### Arrays

PostgreSQL supports arrays of any data type:

**Example:**
```sql
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100),
    email VARCHAR(255),
    tags TEXT[],
    scores INTEGER[]
);

INSERT INTO users (name, email, tags, scores)
VALUES (
    'John Doe',
    'john@example.com',
    ARRAY['developer', 'postgresql'],
    ARRAY[95, 87, 92]
);

-- Query arrays
SELECT * FROM users WHERE 'postgresql' = ANY(tags);
SELECT name, array_length(scores, 1) AS num_scores FROM users;

-- Array functions
SELECT 
    name,
    tags,
    array_length(tags, 1) AS tag_count,
    array_to_string(tags, ', ') AS tags_string,
    unnest(tags) AS individual_tag
FROM users;

-- Array operators
SELECT * FROM users WHERE tags @> ARRAY['postgresql'];  -- contains
SELECT * FROM users WHERE tags && ARRAY['postgresql', 'python'];  -- overlaps
SELECT * FROM users WHERE 'postgresql' = ANY(tags);  -- any element equals
```

### UUID

Universally Unique Identifier (UUID) type:

**Example:**
```sql
-- Enable UUID extension
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";

CREATE TABLE users (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    username VARCHAR(50),
    email VARCHAR(255)
);
```

### Custom Types

PostgreSQL allows creating custom data types:

**Enum Types:**
```sql
-- Create enum type
CREATE TYPE mood AS ENUM ('sad', 'ok', 'happy');

CREATE TABLE person (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100),
    current_mood mood
);

INSERT INTO person (name, current_mood)
VALUES ('Alice', 'happy');
```

**Composite Types:**
```sql
-- Create composite type
CREATE TYPE address AS (
    street VARCHAR(100),
    city VARCHAR(50),
    zip_code VARCHAR(10)
);

CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100),
    home_address address
);

INSERT INTO users (name, home_address)
VALUES ('John', ROW('123 Main St', 'NYC', '10001')::address);

-- Query composite types
SELECT name, (home_address).city FROM users;
```

**Domain Types:**
```sql
-- Create domain (constrained type)
CREATE DOMAIN email_address AS VARCHAR(255)
CHECK (VALUE ~* '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}$');

CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    email email_address
);
```

**hstore (Key-Value Store):**
```sql
-- Enable hstore extension
CREATE EXTENSION IF NOT EXISTS hstore;

CREATE TABLE products (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100),
    attributes hstore
);

INSERT INTO products (name, attributes)
VALUES ('Laptop', 'brand => Dell, ram => 16GB, storage => 512GB');

-- Query hstore
SELECT name, attributes->'brand' AS brand FROM products;
SELECT * FROM products WHERE attributes @> 'ram => 16GB';
```

**XML Type:**
```sql
CREATE TABLE documents (
    id SERIAL PRIMARY KEY,
    title VARCHAR(100),
    content XML
);

INSERT INTO documents (title, content)
VALUES ('Config', '<settings><key>value</key></settings>'::XML);

-- Query XML
SELECT xmlparse(CONTENT '<root>data</root>');
SELECT xpath('//key/text()', content) FROM documents;
```

**BYTEA (Binary Data):**
```sql
CREATE TABLE images (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100),
    data BYTEA
);

-- Insert binary data
INSERT INTO images (name, data) 
VALUES ('photo.jpg', '\x89504E470D0A1A0A...'::bytea);
```

### Sequences and Identity Columns

**SERIAL Types:**
- `SERIAL`: Creates INTEGER column with sequence (1 to 2,147,483,647)
- `BIGSERIAL`: Creates BIGINT column with sequence
- `SMALLSERIAL`: Creates SMALLINT column with sequence

**Identity Columns (PostgreSQL 10+):**
```sql
-- GENERATED ALWAYS AS IDENTITY (cannot override)
CREATE TABLE users (
    id INTEGER GENERATED ALWAYS AS IDENTITY PRIMARY KEY,
    username VARCHAR(50)
);

-- GENERATED BY DEFAULT AS IDENTITY (can override)
CREATE TABLE users (
    id INTEGER GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    username VARCHAR(50)
);

-- Identity column with options
CREATE TABLE users (
    id INTEGER GENERATED BY DEFAULT AS IDENTITY 
        (START WITH 1 INCREMENT BY 1 MAXVALUE 1000) PRIMARY KEY,
    username VARCHAR(50)
);
```

**Manual Sequences:**
```sql
-- Create sequence
CREATE SEQUENCE user_id_seq 
    START WITH 1 
    INCREMENT BY 1 
    MINVALUE 1 
    MAXVALUE 9223372036854775807 
    CACHE 1;

-- Use sequence
CREATE TABLE users (
    id INTEGER DEFAULT nextval('user_id_seq') PRIMARY KEY,
    username VARCHAR(50)
);

-- Sequence functions
SELECT nextval('user_id_seq');      -- Get next value
SELECT currval('user_id_seq');      -- Get current value
SELECT setval('user_id_seq', 100);  -- Set value
SELECT lastval();                   -- Last value from any sequence

-- Alter sequence
ALTER SEQUENCE user_id_seq RESTART WITH 1000;
ALTER SEQUENCE user_id_seq INCREMENT BY 2;
```

## Schema Design

### DDL Commands (Data Definition Language)

DDL commands are used to define and modify database structure. In PostgreSQL, DDL commands are transactional and can be rolled back.

#### CREATE

Create databases, tables, indexes, and other database objects:

**Create Database:**
```sql
CREATE DATABASE mydb
    WITH OWNER = postgres
    ENCODING = 'UTF8'
    LC_COLLATE = 'en_US.UTF-8'
    LC_CTYPE = 'en_US.UTF-8';
```

**Create Table:**
```sql
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    username VARCHAR(50) NOT NULL UNIQUE,
    email VARCHAR(255) NOT NULL UNIQUE,
    password_hash VARCHAR(255) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

**Create Table with Constraints:**
```sql
CREATE TABLE orders (
    id SERIAL PRIMARY KEY,
    user_id INTEGER NOT NULL,
    total DECIMAL(10, 2) NOT NULL CHECK (total >= 0),
    status VARCHAR(20) DEFAULT 'pending' CHECK (status IN ('pending', 'completed', 'cancelled')),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE CASCADE
);
```

#### ALTER

Modify existing database objects:

```sql
-- Add column
ALTER TABLE users ADD COLUMN phone VARCHAR(20);

-- Modify column
ALTER TABLE users ALTER COLUMN email TYPE VARCHAR(100);

-- Rename column
ALTER TABLE users RENAME COLUMN phone TO phone_number;

-- Drop column
ALTER TABLE users DROP COLUMN phone_number;

-- Add constraint
ALTER TABLE users ADD CONSTRAINT check_email_format 
    CHECK (email ~* '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}$');

-- Rename table
ALTER TABLE users RENAME TO accounts;

-- Add column with default
ALTER TABLE users ADD COLUMN phone VARCHAR(20) DEFAULT NULL;

-- Add column with NOT NULL (requires default for existing rows)
ALTER TABLE users ADD COLUMN status VARCHAR(20) NOT NULL DEFAULT 'active';

-- Change column default
ALTER TABLE users ALTER COLUMN status SET DEFAULT 'inactive';

-- Drop column default
ALTER TABLE users ALTER COLUMN status DROP DEFAULT;

-- Set column NOT NULL
ALTER TABLE users ALTER COLUMN email SET NOT NULL;

-- Drop NOT NULL constraint
ALTER TABLE users ALTER COLUMN phone DROP NOT NULL;

-- Change column data type
ALTER TABLE users ALTER COLUMN age TYPE INTEGER USING age::INTEGER;

-- Rename column
ALTER TABLE users RENAME COLUMN phone TO phone_number;

-- Add constraint to existing table
ALTER TABLE users ADD CONSTRAINT check_email_format 
    CHECK (email ~* '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}$');

-- Drop constraint
ALTER TABLE users DROP CONSTRAINT check_email_format;
```

#### DROP

Remove database objects:

```sql
-- Drop table
DROP TABLE users;

-- Drop table with CASCADE (drops dependent objects)
DROP TABLE users CASCADE;

-- Drop database
DROP DATABASE mydb;
```

#### TRUNCATE

Remove all rows from a table:

```sql
-- Truncate table
TRUNCATE TABLE users;

-- Truncate with CASCADE (truncate dependent tables)
TRUNCATE TABLE users CASCADE;

-- Truncate and restart identity
TRUNCATE TABLE users RESTART IDENTITY;
```

### Tables and Constraints

**Primary Key:**
```sql
-- Using SERIAL (auto-increment)
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    username VARCHAR(50)
);

-- Using Identity Column (PostgreSQL 10+)
CREATE TABLE users (
    id INTEGER GENERATED ALWAYS AS IDENTITY PRIMARY KEY,
    username VARCHAR(50)
);

-- Identity column with options
CREATE TABLE users (
    id INTEGER GENERATED BY DEFAULT AS IDENTITY 
        (START WITH 1 INCREMENT BY 1) PRIMARY KEY,
    username VARCHAR(50)
);
```

**Sequences:**
```sql
-- Create sequence
CREATE SEQUENCE user_id_seq START 1 INCREMENT 1;

-- Use sequence
CREATE TABLE users (
    id INTEGER DEFAULT nextval('user_id_seq') PRIMARY KEY,
    username VARCHAR(50)
);

-- Sequence functions
SELECT nextval('user_id_seq');
SELECT currval('user_id_seq');
SELECT setval('user_id_seq', 100);

-- SERIAL creates sequence automatically
-- SERIAL = INTEGER + SEQUENCE + DEFAULT nextval()
```

**Foreign Key:**
```sql
CREATE TABLE orders (
    id SERIAL PRIMARY KEY,
    user_id INTEGER REFERENCES users(id) ON DELETE CASCADE,
    total DECIMAL(10, 2)
);
```

**Check Constraints:**
```sql
CREATE TABLE products (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100),
    price DECIMAL(10, 2) CHECK (price > 0),
    stock INTEGER CHECK (stock >= 0)
);

-- Named check constraint
CREATE TABLE products (
    id SERIAL PRIMARY KEY,
    price DECIMAL(10, 2),
    discount DECIMAL(10, 2),
    CONSTRAINT check_discount CHECK (discount < price)
);
```

**NOT NULL Constraint:**
```sql
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    username VARCHAR(50) NOT NULL,
    email VARCHAR(255) NOT NULL,
    phone VARCHAR(20)  -- nullable
);
```

**DEFAULT Constraint:**
```sql
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    username VARCHAR(50) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    status VARCHAR(20) DEFAULT 'active',
    is_premium BOOLEAN DEFAULT FALSE
);
```

**UNIQUE Constraint:**
```sql
-- Column-level unique
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    username VARCHAR(50) UNIQUE,
    email VARCHAR(255) UNIQUE
);

-- Table-level unique (multiple columns)
CREATE TABLE user_sessions (
    id SERIAL PRIMARY KEY,
    user_id INTEGER,
    session_token VARCHAR(255),
    UNIQUE (user_id, session_token)
);
```

**Unique Constraints:**
```sql
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    username VARCHAR(50) UNIQUE,
    email VARCHAR(255) UNIQUE
);
```

**NOT NULL Constraints:**
```sql
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    username VARCHAR(50) NOT NULL,
    email VARCHAR(255) NOT NULL
);
```

### Indexes

PostgreSQL supports various index types:

**B-tree Index (Default):**
```sql
CREATE INDEX idx_users_email ON users(email);
CREATE INDEX idx_users_created ON users(created_at DESC);
```

**Composite Index:**
```sql
CREATE INDEX idx_users_name_email ON users(last_name, first_name, email);
```

**Partial Index:**
```sql
CREATE INDEX idx_active_users ON users(email) WHERE is_active = TRUE;
```

**Unique Index:**
```sql
CREATE UNIQUE INDEX idx_users_username ON users(username);
```

**GIN Index (for JSONB, arrays, full-text search):**
```sql
CREATE INDEX idx_products_attributes ON products USING GIN (attributes);
```

**GiST Index (for geometric data, full-text search):**
```sql
CREATE INDEX idx_locations_geom ON locations USING GIST (geom);
```

**BRIN Index (for large tables with sorted data):**
```sql
CREATE INDEX idx_logs_timestamp ON logs USING BRIN (timestamp);
```

### Foreign Keys

**Foreign Key with Actions:**
```sql
CREATE TABLE orders (
    id SERIAL PRIMARY KEY,
    user_id INTEGER NOT NULL,
    total DECIMAL(10, 2),
    FOREIGN KEY (user_id) REFERENCES users(id)
        ON DELETE CASCADE      -- Delete orders when user is deleted
        ON UPDATE CASCADE      -- Update order.user_id when user.id changes
);
```

**Other Foreign Key Actions:**
- `ON DELETE RESTRICT`: Prevent deletion if referenced
- `ON DELETE SET NULL`: Set foreign key to NULL
- `ON DELETE SET DEFAULT`: Set foreign key to default value
- `ON DELETE NO ACTION`: Similar to RESTRICT (checked at end of transaction)

**DELETE CASCADE Example:**
```sql
-- When parent row is deleted, child rows are automatically deleted
CREATE TABLE orders (
    id SERIAL PRIMARY KEY,
    user_id INTEGER REFERENCES users(id) ON DELETE CASCADE
);

-- Deleting a user will automatically delete all their orders
DELETE FROM users WHERE id = 1;  -- Also deletes related orders
```

### Schemas

PostgreSQL uses schemas to organize database objects:

```sql
-- Create schema
CREATE SCHEMA myschema;

-- Create table in schema
CREATE TABLE myschema.users (id SERIAL PRIMARY KEY);

-- Set search path
SET search_path TO myschema, public;

-- Drop schema
DROP SCHEMA myschema CASCADE;
```

## SQL Fundamentals

### Introduction to SQL

SQL (Structured Query Language) is the standard language for accessing and manipulating relational databases. PostgreSQL implements SQL with some PostgreSQL-specific extensions.

**Types of SQL Commands:**
- **DDL (Data Definition Language)**: CREATE, ALTER, DROP, TRUNCATE
- **DML (Data Manipulation Language)**: INSERT, UPDATE, DELETE, SELECT
- **DCL (Data Control Language)**: GRANT, REVOKE
- **TCL (Transaction Control Language)**: BEGIN, COMMIT, ROLLBACK, SAVEPOINT

### Querying Data

#### SELECT Statement

**Basic SELECT:**
```sql
-- Select all columns
SELECT * FROM users;

-- Select specific columns
SELECT username, email FROM users;

-- Select with column alias
SELECT username AS name, email AS email_address FROM users;
SELECT username name, email email_address FROM users;  -- AS is optional
```

**SELECT DISTINCT:**
```sql
-- Remove duplicate rows
SELECT DISTINCT city FROM users;

-- DISTINCT on multiple columns
SELECT DISTINCT city, country FROM users;

-- DISTINCT ON (PostgreSQL specific)
SELECT DISTINCT ON (city) city, username, email
FROM users
ORDER BY city, created_at DESC;
```

**Column Aliases:**
```sql
-- Column aliases make queries more readable
SELECT 
    username AS user_name,
    email AS email_address,
    created_at AS registration_date
FROM users;

-- Aliases in expressions
SELECT 
    total,
    total * 0.1 AS tax,
    total * 1.1 AS total_with_tax
FROM orders;
```

**Table Aliases:**
```sql
-- Table aliases for shorter syntax
SELECT u.username, o.total
FROM users u
INNER JOIN orders o ON u.id = o.user_id;

-- Table aliases are required for self-joins
SELECT 
    e1.name AS employee,
    e2.name AS manager
FROM employees e1
LEFT JOIN employees e2 ON e1.manager_id = e2.id;
```

#### WHERE Clause

```sql
-- Equality
SELECT * FROM users WHERE id = 1;

-- Comparison operators
SELECT * FROM users WHERE age >= 18;
SELECT * FROM users WHERE created_at > '2024-01-01';

-- Logical operators
SELECT * FROM users WHERE age >= 18 AND status = 'active';
SELECT * FROM users WHERE city = 'NYC' OR city = 'LA';
SELECT * FROM users WHERE NOT is_deleted;

-- IN operator
SELECT * FROM users WHERE id IN (1, 2, 3, 4, 5);

-- BETWEEN operator
SELECT * FROM users WHERE age BETWEEN 18 AND 65;
SELECT * FROM users WHERE created_at BETWEEN '2024-01-01' AND '2024-12-31';

-- IS NULL / IS NOT NULL
SELECT * FROM users WHERE email IS NOT NULL;
```

#### ORDER BY

```sql
-- Sort ascending (default)
SELECT * FROM users ORDER BY created_at;

-- Sort descending
SELECT * FROM users ORDER BY created_at DESC;

-- Multiple columns
SELECT * FROM users ORDER BY last_name ASC, first_name ASC;

-- NULLS FIRST / NULLS LAST
SELECT * FROM users ORDER BY email NULLS LAST;
```

#### LIMIT and OFFSET

```sql
-- Limit number of rows
SELECT * FROM users LIMIT 10;

-- Pagination
SELECT * FROM users LIMIT 10 OFFSET 20;

-- Using FETCH (SQL standard)
SELECT * FROM users FETCH FIRST 10 ROWS ONLY;

-- FETCH with OFFSET
SELECT * FROM users 
ORDER BY created_at DESC
OFFSET 20 ROWS
FETCH FIRST 10 ROWS ONLY;
```

**SELECT INTO:**
```sql
-- Create table from SELECT result
SELECT * INTO users_backup FROM users WHERE created_at < '2024-01-01';

-- Create table as (alternative syntax)
CREATE TABLE users_backup AS 
SELECT * FROM users WHERE created_at < '2024-01-01';
```

### SQL Operators

#### Comparison Operators

- `=`: Equal
- `<>` or `!=`: Not equal
- `<`: Less than
- `>`: Greater than
- `<=`: Less than or equal
- `>=`: Greater than or equal

#### Logical Operators

- `AND`: Both conditions must be true
- `OR`: At least one condition must be true
- `NOT`: Negates a condition

#### Pattern Matching

**LIKE Operator:**
```sql
-- % matches any sequence of characters
SELECT * FROM users WHERE email LIKE '%@gmail.com';

-- _ matches any single character
SELECT * FROM users WHERE username LIKE 'j_hn';

-- ILIKE (case-insensitive LIKE)
SELECT * FROM users WHERE name ILIKE 'john%';
```

**Regular Expressions:**
```sql
-- ~ (case-sensitive regex match)
SELECT * FROM users WHERE email ~ '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}$';

-- ~* (case-insensitive regex match)
SELECT * FROM users WHERE name ~* '^john';

-- !~ (does not match)
SELECT * FROM users WHERE email !~ '^[^@]+@[^@]+\.[^@]+$';
```

### DML Commands (Data Manipulation Language)

#### INSERT Statement

```sql
-- Insert single row
INSERT INTO users (username, email, password_hash)
VALUES ('john_doe', 'john@example.com', 'hashed_password');

-- Insert multiple rows
INSERT INTO users (username, email) VALUES
    ('alice', 'alice@example.com'),
    ('bob', 'bob@example.com'),
    ('charlie', 'charlie@example.com');

-- Insert with RETURNING
INSERT INTO users (username, email)
VALUES ('dave', 'dave@example.com')
RETURNING id, username, created_at;

-- Insert from SELECT
INSERT INTO users_backup (username, email)
SELECT username, email FROM users WHERE created_at < '2024-01-01';
```

#### UPDATE Statement

```sql
-- Update single row
UPDATE users SET email = 'newemail@example.com' WHERE id = 1;

-- Update multiple columns
UPDATE users 
SET email = 'newemail@example.com', updated_at = CURRENT_TIMESTAMP
WHERE id = 1;

-- Update with subquery
UPDATE orders 
SET total = (
    SELECT SUM(price * quantity) 
    FROM order_items 
    WHERE order_id = orders.id
)
WHERE id = 1;

-- Update with RETURNING
UPDATE users SET status = 'active' WHERE id = 1
RETURNING id, username, status;
```

**UPDATE with JOIN:**
```sql
-- Update based on values in another table
UPDATE orders o
SET status = 'completed'
FROM users u
WHERE o.user_id = u.id
AND u.status = 'premium'
AND o.created_at < '2024-01-01';

-- Alternative syntax using subquery
UPDATE orders
SET status = 'completed'
WHERE user_id IN (
    SELECT id FROM users WHERE status = 'premium'
)
AND created_at < '2024-01-01';
```

#### DELETE Statement

```sql
-- Delete specific rows
DELETE FROM users WHERE id = 1;

-- Delete all rows (use TRUNCATE for better performance)
DELETE FROM users;

-- Delete with subquery
DELETE FROM orders 
WHERE user_id IN (
    SELECT id FROM users WHERE status = 'deleted'
);

-- Delete with RETURNING
DELETE FROM users WHERE id = 1
RETURNING id, username;
```

#### UPSERT (INSERT ... ON CONFLICT)

PostgreSQL's unique feature for handling conflicts:

```sql
-- Insert or update on conflict
INSERT INTO users (id, username, email)
VALUES (1, 'john_doe', 'john@example.com')
ON CONFLICT (id) 
DO UPDATE SET 
    username = EXCLUDED.username,
    email = EXCLUDED.email,
    updated_at = CURRENT_TIMESTAMP;

-- Insert or do nothing on conflict
INSERT INTO users (id, username, email)
VALUES (1, 'john_doe', 'john@example.com')
ON CONFLICT (id) DO NOTHING;

-- Upsert on unique constraint
INSERT INTO users (username, email)
VALUES ('john_doe', 'john@example.com')
ON CONFLICT (username) 
DO UPDATE SET email = EXCLUDED.email;
```

### Joins

#### INNER JOIN

Returns rows that have matching values in both tables:

```sql
SELECT users.username, orders.total
FROM users
INNER JOIN orders ON users.id = orders.user_id;
```

#### LEFT JOIN

Returns all rows from left table and matched rows from right table:

```sql
SELECT users.username, orders.total
FROM users
LEFT JOIN orders ON users.id = orders.user_id;
```

#### RIGHT JOIN

Returns all rows from right table and matched rows from left table:

```sql
SELECT users.username, orders.total
FROM users
RIGHT JOIN orders ON users.id = orders.user_id;
```

#### FULL OUTER JOIN

Returns all rows from both tables:

```sql
SELECT users.username, orders.total
FROM users
FULL OUTER JOIN orders ON users.id = orders.user_id;
```

#### CROSS JOIN

Cartesian product of both tables:

```sql
SELECT * FROM users CROSS JOIN products;
```

#### Self-Join

Join a table to itself:

```sql
-- Find employees and their managers
SELECT 
    e1.name AS employee,
    e2.name AS manager
FROM employees e1
LEFT JOIN employees e2 ON e1.manager_id = e2.id;

-- Find users who registered on the same day
SELECT 
    u1.username AS user1,
    u2.username AS user2,
    u1.created_at::DATE AS registration_date
FROM users u1
INNER JOIN users u2 ON u1.created_at::DATE = u2.created_at::DATE
WHERE u1.id < u2.id;
```

#### Natural Join

Join tables using implicit join conditions based on common column names:

```sql
-- Natural join (matches columns with same name)
SELECT * FROM users NATURAL JOIN orders;

-- Equivalent to:
SELECT * FROM users 
INNER JOIN orders ON users.id = orders.id;
```

**Note:** Natural joins can be dangerous if column names don't match as expected. Explicit JOIN syntax is recommended.

### Aggregations and Grouping

```sql
-- COUNT
SELECT COUNT(*) FROM users;
SELECT COUNT(DISTINCT city) FROM users;

-- SUM, AVG, MIN, MAX
SELECT 
    SUM(total) AS total_revenue,
    AVG(total) AS avg_order,
    MIN(total) AS min_order,
    MAX(total) AS max_order
FROM orders;

-- GROUP BY
SELECT city, COUNT(*) AS user_count
FROM users
GROUP BY city;

-- HAVING (filter groups)
SELECT city, COUNT(*) AS user_count
FROM users
GROUP BY city
HAVING COUNT(*) > 10;

-- Multiple grouping columns
SELECT city, status, COUNT(*) AS count
FROM users
GROUP BY city, status;
```

**Grouping Sets:**
```sql
-- Multiple grouping sets
SELECT 
    city,
    status,
    COUNT(*) AS count
FROM users
GROUP BY GROUPING SETS (
    (city, status),
    (city),
    (status),
    ()
);

-- Equivalent to multiple UNION ALL queries
```

**CUBE:**
```sql
-- Generate all possible grouping combinations
SELECT 
    city,
    status,
    COUNT(*) AS count
FROM users
GROUP BY CUBE (city, status);

-- Generates: (city, status), (city), (status), ()
```

**ROLLUP:**
```sql
-- Generate hierarchical grouping (totals and subtotals)
SELECT 
    year,
    quarter,
    month,
    SUM(revenue) AS total_revenue
FROM sales
GROUP BY ROLLUP (year, quarter, month);

-- Generates: (year, quarter, month), (year, quarter), (year), ()
```

**Set Operations:**
```sql
-- UNION (combine result sets, removes duplicates)
SELECT username FROM users WHERE city = 'NYC'
UNION
SELECT username FROM users WHERE city = 'LA';

-- UNION ALL (keeps duplicates)
SELECT username FROM users WHERE city = 'NYC'
UNION ALL
SELECT username FROM users WHERE city = 'LA';

-- INTERSECT (rows in both result sets)
SELECT username FROM users WHERE city = 'NYC'
INTERSECT
SELECT username FROM users WHERE age > 25;

-- EXCEPT (rows in first but not in second)
SELECT username FROM users WHERE city = 'NYC'
EXCEPT
SELECT username FROM users WHERE age < 18;
```

### Window Functions

Window functions perform calculations across a set of rows related to the current row:

```sql
-- ROW_NUMBER
SELECT 
    username,
    total,
    ROW_NUMBER() OVER (ORDER BY total DESC) AS rank
FROM orders;

-- RANK and DENSE_RANK
SELECT 
    username,
    total,
    RANK() OVER (ORDER BY total DESC) AS rank,
    DENSE_RANK() OVER (ORDER BY total DESC) AS dense_rank
FROM orders;

-- PARTITION BY
SELECT 
    user_id,
    total,
    SUM(total) OVER (PARTITION BY user_id) AS user_total
FROM orders;

-- LAG and LEAD
SELECT 
    date,
    revenue,
    LAG(revenue) OVER (ORDER BY date) AS prev_revenue,
    LEAD(revenue) OVER (ORDER BY date) AS next_revenue
FROM daily_sales;
```

### Common Table Expressions (CTEs)

CTEs provide a way to write auxiliary statements for use in a larger query:

```sql
-- Simple CTE
WITH active_users AS (
    SELECT * FROM users WHERE status = 'active'
)
SELECT * FROM active_users WHERE created_at > '2024-01-01';

-- Multiple CTEs
WITH 
    user_totals AS (
        SELECT user_id, SUM(total) AS total_spent
        FROM orders
        GROUP BY user_id
    ),
    top_users AS (
        SELECT user_id FROM user_totals
        ORDER BY total_spent DESC
        LIMIT 10
    )
SELECT u.username, ut.total_spent
FROM users u
INNER JOIN user_totals ut ON u.id = ut.user_id
WHERE u.id IN (SELECT user_id FROM top_users);

-- Recursive CTE
WITH RECURSIVE category_tree AS (
    -- Base case
    SELECT id, name, parent_id, 0 AS level
    FROM categories
    WHERE parent_id IS NULL
    
    UNION ALL
    
    -- Recursive case
    SELECT c.id, c.name, c.parent_id, ct.level + 1
    FROM categories c
    INNER JOIN category_tree ct ON c.parent_id = ct.id
)
SELECT * FROM category_tree;
```

### Subqueries

```sql
-- Scalar subquery
SELECT username, 
       (SELECT COUNT(*) FROM orders WHERE user_id = users.id) AS order_count
FROM users;

-- Subquery in WHERE
SELECT * FROM users
WHERE id IN (SELECT user_id FROM orders WHERE total > 100);

-- EXISTS subquery
SELECT * FROM users
WHERE EXISTS (
    SELECT 1 FROM orders 
    WHERE orders.user_id = users.id AND orders.total > 100
);

-- Correlated subquery
SELECT username,
       (SELECT MAX(total) FROM orders WHERE user_id = users.id) AS max_order
FROM users;
```

**ANY and ALL Operators:**
```sql
-- ANY: returns true if comparison is true for any value in subquery
SELECT * FROM products
WHERE price > ANY (
    SELECT price FROM products WHERE category = 'premium'
);

-- Equivalent to:
SELECT * FROM products
WHERE price > (SELECT MIN(price) FROM products WHERE category = 'premium');

-- ALL: returns true if comparison is true for all values in subquery
SELECT * FROM products
WHERE price > ALL (
    SELECT price FROM products WHERE category = 'basic'
);

-- Equivalent to:
SELECT * FROM products
WHERE price > (SELECT MAX(price) FROM products WHERE category = 'basic');
```

### Views

Views are virtual tables based on the result of a SQL statement:

```sql
-- Create view
CREATE VIEW active_users AS
SELECT id, username, email, created_at
FROM users
WHERE status = 'active';

-- Query view
SELECT * FROM active_users;

-- Updatable view
CREATE VIEW user_emails AS
SELECT id, username, email
FROM users;

-- Can INSERT/UPDATE/DDELETE if view has single table and no aggregates
INSERT INTO user_emails (username, email) VALUES ('newuser', 'new@example.com');

-- Materialized view (stores data physically)
CREATE MATERIALIZED VIEW user_order_summary AS
SELECT 
    u.id,
    u.username,
    COUNT(o.id) AS order_count,
    SUM(o.total) AS total_spent
FROM users u
LEFT JOIN orders o ON u.id = o.user_id
GROUP BY u.id, u.username;

-- Refresh materialized view
REFRESH MATERIALIZED VIEW user_order_summary;

-- Concurrent refresh (doesn't lock view)
REFRESH MATERIALIZED VIEW CONCURRENTLY user_order_summary;

-- Drop view
DROP VIEW active_users;
```

### Conditional Expressions & Operators

**CASE Expression:**
```sql
-- Simple CASE
SELECT 
    username,
    CASE status
        WHEN 'active' THEN 'Active User'
        WHEN 'inactive' THEN 'Inactive User'
        ELSE 'Unknown'
    END AS status_description
FROM users;

-- Searched CASE
SELECT 
    total,
    CASE
        WHEN total > 1000 THEN 'High Value'
        WHEN total > 500 THEN 'Medium Value'
        ELSE 'Low Value'
    END AS order_category
FROM orders;
```

**COALESCE:**
```sql
-- Return first non-null value
SELECT 
    username,
    COALESCE(display_name, username) AS display_name,
    COALESCE(phone, email, 'No contact') AS contact
FROM users;
```

**NULLIF:**
```sql
-- Return NULL if two values are equal
SELECT 
    username,
    NULLIF(email, '') AS email  -- NULL if email is empty string
FROM users;

-- Prevent division by zero
SELECT 
    total,
    quantity,
    NULLIF(quantity, 0) AS safe_quantity,
    total / NULLIF(quantity, 0) AS unit_price
FROM order_items;
```

**CAST:**
```sql
-- Type casting
SELECT CAST('123' AS INTEGER);
SELECT '123'::INTEGER;  -- PostgreSQL shorthand

-- Common casts
SELECT 
    CAST('2024-01-01' AS DATE),
    CAST(123.45 AS DECIMAL(10, 2)),
    CAST(123 AS VARCHAR);
```

## Transactions & Concurrency

### Transactions and ACID Properties

PostgreSQL ensures ACID (Atomicity, Consistency, Isolation, Durability) properties:

- **Atomicity**: All operations in a transaction succeed or fail together
- **Consistency**: Database remains in a valid state
- **Isolation**: Concurrent transactions don't interfere with each other
- **Durability**: Committed changes persist even after system failure

### Transaction Control

```sql
-- Begin transaction
BEGIN;

-- Or
BEGIN TRANSACTION;

-- Commit transaction
COMMIT;

-- Rollback transaction
ROLLBACK;

-- Savepoint
BEGIN;
SAVEPOINT my_savepoint;
-- ... some operations ...
ROLLBACK TO SAVEPOINT my_savepoint;
COMMIT;
```

### Isolation Levels

PostgreSQL supports four isolation levels (from least to most strict):

1. **READ UNCOMMITTED**: Not supported (treated as READ COMMITTED)
2. **READ COMMITTED**: Default level, sees only committed data
3. **REPEATABLE READ**: Sees snapshot of data at transaction start
4. **SERIALIZABLE**: Highest isolation, prevents all anomalies

```sql
-- Set isolation level
SET TRANSACTION ISOLATION LEVEL SERIALIZABLE;

-- Or in transaction
BEGIN ISOLATION LEVEL REPEATABLE READ;
```

### MVCC (Multi-Version Concurrency Control)

PostgreSQL uses MVCC to provide concurrent access:

- Each transaction sees a snapshot of the database
- No read locks required
- Writers don't block readers
- Readers don't block writers

### Locking

```sql
-- Table-level locks
LOCK TABLE users IN SHARE MODE;
LOCK TABLE users IN EXCLUSIVE MODE;

-- Row-level locks (automatic in UPDATE/DELETE)
SELECT * FROM users WHERE id = 1 FOR UPDATE;
SELECT * FROM users WHERE id = 1 FOR SHARE;

-- Advisory locks
SELECT pg_advisory_lock(123);
SELECT pg_advisory_unlock(123);
```

### Deadlocks

PostgreSQL automatically detects and resolves deadlocks:

```sql
-- Monitor locks
SELECT * FROM pg_locks;

-- Check for blocking queries
SELECT 
    blocked_locks.pid AS blocked_pid,
    blocking_locks.pid AS blocking_pid,
    blocked_activity.query AS blocked_query,
    blocking_activity.query AS blocking_query
FROM pg_catalog.pg_locks blocked_locks
JOIN pg_catalog.pg_stat_activity blocked_activity ON blocked_activity.pid = blocked_locks.pid
JOIN pg_catalog.pg_locks blocking_locks ON blocking_locks.locktype = blocked_locks.locktype
JOIN pg_catalog.pg_stat_activity blocking_activity ON blocking_activity.pid = blocking_locks.pid
WHERE NOT blocked_locks.granted;
```

## High Availability Overview

High availability (HA) ensures that your PostgreSQL database remains operational and accessible, even in the event of component failures. HA is the measure of system resiliency in response to underlying infrastructure failure.

### When to Consider HA Architecture

Use an HA architecture when you want to provide increased data-tier uptime to meet the reliability requirements for your workloads and services. Consider HA when:

- Your service level objectives (SLOs) require high uptime
- You need protection against single zone or regional failures
- Your business cannot tolerate extended downtime
- You require automated failover capabilities

### HA Requirements Assessment

Before selecting an HA architecture, consider:

1. **Availability Level**: Do you require protection against single zone or complete regional failure?
2. **Business Impact**: What is the cost to your business if there is downtime?
3. **Operational Budget**: HA increases infrastructure and storage costs
4. **Recovery Time Objective (RTO)**: How quickly do you need to failover?
5. **Recovery Point Objective (RPO)**: Can you afford to lose data as a result of failover?

## HA Architectures

This section describes three primary HA architectures for PostgreSQL on Compute Engine (or similar infrastructure):

1. **Patroni-based HA**: Mature, feature-rich solution using Patroni and a distributed configuration store
2. **pg_auto_failover**: Simpler solution with built-in monitor service
3. **Stateful MIGs with Regional Persistent Disk**: Google Cloud-native approach using regional persistent disks

### Patroni-based HA

[Patroni](https://github.com/zalando/patroni) is a mature and actively maintained, open-source (MIT licensed) software template that provides tools to configure, deploy, and operate a PostgreSQL HA architecture.

#### Architecture Components

- **Patroni Agent**: Runs on each PostgreSQL node, manages PostgreSQL process and configuration
- **Distributed Configuration Store (DCS)**: Stores cluster state and configuration
  - Options: etcd, Consul, Apache ZooKeeper, or Kubernetes
- **Load Balancer**: Routes traffic to primary and replica nodes (HAProxy or internal passthrough Network Load Balancer)

![Patroni Cluster Components](../assets/postgresql/patroni-components.png)

*Figure 1: Patroni cluster components showing interaction between PostgreSQL nodes, the DCS, and Patroni agents. Image credit: [Architectures for high availability of PostgreSQL clusters on Compute Engine - Google Cloud](https://docs.cloud.google.com/architecture/architectures-high-availability-postgresql-clusters-compute-engine)*

![Patroni Healthy Cluster](../assets/postgresql/patroni-leader-lock.png)

*Figure 2: Healthy cluster leader updates the leader lock while leader candidates watch. Image credit: [Architectures for high availability of PostgreSQL clusters on Compute Engine - Google Cloud](https://docs.cloud.google.com/architecture/architectures-high-availability-postgresql-clusters-compute-engine)*

#### How Patroni Works

1. **Leader Election**: The primary node regularly updates a leader key in the DCS with a TTL. If the TTL elapses without an update, leader election begins.

2. **Failure Detection**: 
   - Patroni agent continuously updates its health status in the DCS
   - Validates PostgreSQL health
   - Self-fences or demotes if issues detected
   - OS-level watchdog prevents split-brain conditions

![Patroni Impaired Cluster](../assets/postgresql/patroni-impaired-cluster.png)

*Figure 3: Impaired cluster elects a new leader after the existing leader key expires. Image credit: [Architectures for high availability of PostgreSQL clusters on Compute Engine - Google Cloud](https://docs.cloud.google.com/architecture/architectures-high-availability-postgresql-clusters-compute-engine)*

3. **Failover Process**:
   - When leader lock expires, replica nodes check their WAL positions
   - Most up-to-date node attempts to acquire leader lock
   - First node to successfully create leader key becomes new primary
   - Other nodes serve as replicas

![Patroni WAL Log Positions](../assets/postgresql/patroni-wal-log-positions.png)

*Figure 4: During the Patroni failover process, replicas check their position in the WAL log. Image credit: [Architectures for high availability of PostgreSQL clusters on Compute Engine - Google Cloud](https://docs.cloud.google.com/architecture/architectures-high-availability-postgresql-clusters-compute-engine)*

![Patroni Leader Race](../assets/postgresql/patroni-leader-race.png)

*Figure 5: A node creates a leader key in the DCS and becomes the new primary. Image credit: [Architectures for high availability of PostgreSQL clusters on Compute Engine - Google Cloud](https://docs.cloud.google.com/architecture/architectures-high-availability-postgresql-clusters-compute-engine)*

4. **Query Routing**: 
   - Patroni exposes REST API endpoints (`/primary`, `/replica`)
   - Load balancer uses these endpoints for health checks
   - Primary: `/primary` returns 200, `/replica` returns 503
   - Replica: `/primary` returns 503, `/replica` returns 200

5. **Fallback**: When a failed node restarts, it detects it doesn't have the leader lock and automatically demotes itself to a replica.

#### Advantages

- Extremely configurable: supports both synchronous and asynchronous replication
- Allows multiple zone and multi-region HA setups
- Feature-rich: hooks, different DCS options, cascaded replicas
- Mature: Used in production by large companies like Zalando and GitLab since 2015

#### Disadvantages

- Requires separate DCS cluster (etcd, Consul, etc.)
- Higher cost due to multiple replicas and DCS infrastructure
- More complex setup and maintenance

#### Patroni Deployment Types

The [PostgreSQL High-Availability Cluster (gecio/postgresql_cluster)](https://github.com/gecio/postgresql_cluster) repository provides Ansible playbooks for deploying Patroni-based HA clusters. The repository includes three deployment architecture types:

![Patroni Type A Architecture](../assets/postgresql/patroni-type-a.png)

*Figure 10: Patroni Type A deployment architecture. Image credit: [PostgreSQL High-Availability Cluster (gecio/postgresql_cluster) - GitHub](https://github.com/gecio/postgresql_cluster)*

![Patroni Type B Architecture](../assets/postgresql/patroni-type-b.png)

*Figure 11: Patroni Type B deployment architecture. Image credit: [PostgreSQL High-Availability Cluster (gecio/postgresql_cluster) - GitHub](https://github.com/gecio/postgresql_cluster)*

![Patroni Type C Architecture](../assets/postgresql/patroni-type-c.png)

*Figure 12: Patroni Type C deployment architecture. Image credit: [PostgreSQL High-Availability Cluster (gecio/postgresql_cluster) - GitHub](https://github.com/gecio/postgresql_cluster)*

![Load Balancing Configuration](../assets/postgresql/load-balancing.jpg)

*Figure 13: Load balancing configuration for PostgreSQL HA clusters. Image credit: [PostgreSQL High-Availability Cluster (gecio/postgresql_cluster) - GitHub](https://github.com/gecio/postgresql_cluster)*

### pg_auto_failover

[pg_auto_failover](https://github.com/citusdata/pg_auto_failover) is an actively developed, open-source (PostgreSQL license) PostgreSQL extension that configures HA by extending existing PostgreSQL capabilities.

#### Architecture Components

- **Monitor Service**: PostgreSQL instance with pg_auto_failover extension that maintains global state
- **Keeper Agent**: Process on each data node that observes and manages PostgreSQL service
- **Formation**: Collection of nodes managed by pg_auto_failover (minimum 3 nodes)

![pg_auto_failover Architecture](../assets/postgresql/pg-auto-failover-architecture.png)

*Figure 6: pg_auto_failover architecture containing a formation of nodes. Image credit: [Architectures for high availability of PostgreSQL clusters on Compute Engine - Google Cloud](https://docs.cloud.google.com/architecture/architectures-high-availability-postgresql-clusters-compute-engine)*

#### How pg_auto_failover Works

1. **Monitor Service**: 
   - Maintains global state for the formation
   - Obtains health check status from member nodes
   - Orchestrates group using finite state machine (FSM) rules

2. **Keeper Agent**: 
   - Observes and manages PostgreSQL service on each node
   - Sends status updates to Monitor
   - Receives and executes actions from Monitor

3. **Failure Detection**:
   - Keeper agents periodically connect to Monitor
   - Monitor performs health checks using PostgreSQL protocol
   - If neither action succeeds after 30 seconds (default), failure detected

![pg_auto_failover Failure Scenarios](../assets/postgresql/pg-auto-failover-failure-scenarios.png)

*Figure 7: pg_auto_failover failure scenarios for primary, secondary, and monitor node failures. Image credit: [Architectures for high availability of PostgreSQL clusters on Compute Engine - Google Cloud](https://docs.cloud.google.com/architecture/architectures-high-availability-postgresql-clusters-compute-engine)*

4. **Failover Process**:
   - Secondary nodes considered for promotion based on:
     - Highest `candidate_priority` (0-100, default 50)
     - Most advanced WAL log position
     - Random selection as tie-breaker
   - Lagging candidates fetch missing WAL from most advanced standby

5. **Query Routing**: 
   - Requires client library support for multiple hosts
   - Uses libpq with multiple host definitions
   - Not easily fronted with a load balancer

#### Advantages

- No external dependencies other than PostgreSQL
- Simpler than Patroni
- Automatic node initialization
- Similar configurability to Patroni

#### Disadvantages

- Monitor node is a single point of failure (requires HA/DR planning)
- Requires client-side query routing (not transparent)
- Relatively new project (announced early 2019)
- Limited to single Monitor instance

### Stateful MIGs with Regional Persistent Disk

This approach uses Google Cloud components exclusively, providing a simpler HA solution for cloud-native deployments.

#### Architecture Components

- **Regional Persistent Disk**: Synchronously replicates data between two zones in a region
- **Stateful Managed Instance Groups (MIGs)**: Pair of MIGs keep one primary node running
- **Cloud Storage**: Stores configuration indicating which MIG is running primary
- **MIG Health Checks**: Monitor instance health and trigger autohealing
- **Cloud Run Functions**: Event-driven functions triggered by Pub/Sub for failover
- **Internal Passthrough Network Load Balancer**: Routes clients to running instance

![Stateful MIG with Regional Persistent Disk](../assets/postgresql/stateful-mig-regional-pd.svg)

*Figure 8: HA using stateful MIGs and regional persistent disks. Image credit: [Architectures for high availability of PostgreSQL clusters on Compute Engine - Google Cloud](https://docs.cloud.google.com/architecture/architectures-high-availability-postgresql-clusters-compute-engine)*

#### How It Works

1. **Regional Persistent Disk**: 
   - Data synchronously replicated between two zones
   - Only one active PostgreSQL node at a time
   - Force-attach operation completes in < 1 minute

2. **Failure Detection & Failover**:
   - Health check monitors node health
   - Failed health check stops unhealthy instance
   - Logging exports entry to Pub/Sub
   - Cloud Run function reads config from Cloud Storage
   - Function creates replacement instance in alternate zone
   - Regional persistent disk attached to new instance

![Zonal Failure Replacement](../assets/postgresql/zonal-failure-replacement.svg)

*Figure 9: During a zonal failure, a replacement instance is started. Image credit: [Architectures for high availability of PostgreSQL clusters on Compute Engine - Google Cloud](https://docs.cloud.google.com/architecture/architectures-high-availability-postgresql-clusters-compute-engine)*

3. **Query Routing**: 
   - Load balancer routes to running instance
   - Uses same health check as instance group
   - Connections fail during recreation, resume after instance is up

#### Advantages

- Composed entirely of Google Cloud products
- Low cost: only one active node at a time
- Transparent to client (connects to load balancer)
- Simple architecture

#### Disadvantages

- Limited to two zones in a single region
- Limited scalability (only one active node)
- No read scaling (no read-only replicas)
- Not applicable for multi-region setups

## Replication

### Streaming Replication

Streaming replication is a replication approach where the replica connects to the primary and continuously receives a stream of WAL records. This keeps replicas more up-to-date compared to log-shipping replication.

PostgreSQL offers built-in streaming replication beginning in version 9. Many PostgreSQL HA solutions use this built-in streaming replication to keep multiple replica nodes in sync with the primary.

#### How Streaming Replication Works

1. **WAL Records**: Transactions create WAL records appended to the WAL file
2. **Replication Connection**: Replica connects to primary using replication protocol
3. **WAL Streaming**: Primary streams WAL records to replica in real-time
4. **Apply Changes**: Replica applies WAL records to its database files

#### Configuration

**Primary (postgresql.conf)**:
```ini
wal_level = replica              # or 'logical' for logical replication
max_wal_senders = 3              # number of replicas
wal_keep_segments = 32           # or use replication slots
```

**Primary (pg_hba.conf)**:
```
host replication replicator 192.168.1.0/24 md5
```

**Replica (recovery.conf or postgresql.auto.conf)**:
```ini
primary_conninfo = 'host=primary.example.com port=5432 user=replicator'
primary_slot_name = 'replica1'  # optional, for replication slots
```

### Logical Replication

Logical replication replicates data changes based on replication identity (typically primary key), allowing more control than physical replication.

**Use Cases:**
- Replicating specific tables or databases
- Upgrading PostgreSQL versions
- Cross-version replication
- Selective replication

**Setup Logical Replication:**

**Primary (postgresql.conf):**
```ini
wal_level = logical
max_replication_slots = 10
max_wal_senders = 10
```

**Create Publication (Primary):**
```sql
-- Create publication for all tables
CREATE PUBLICATION my_publication FOR ALL TABLES;

-- Or for specific tables
CREATE PUBLICATION my_publication FOR TABLE users, orders;

-- Add tables to publication
ALTER PUBLICATION my_publication ADD TABLE products;
```

**Create Subscription (Replica):**
```sql
-- Create subscription
CREATE SUBSCRIPTION my_subscription
CONNECTION 'host=primary.example.com port=5432 user=replicator dbname=mydb'
PUBLICATION my_publication;

-- View subscription status
SELECT * FROM pg_subscription;
SELECT * FROM pg_stat_subscription;
```

### Synchronous vs Asynchronous

#### Asynchronous Replication (Default)

- Primary doesn't wait for replica confirmation before committing
- Lower latency
- Risk of data loss if primary crashes before replica receives transaction

#### Synchronous Replication

- Primary waits for replica confirmation before committing
- Higher durability
- Higher transaction latency
- Configurable with `synchronous_commit` and `synchronous_standby_names`

**synchronous_commit Options**:

- `local`: No standby involvement (default for async)
- `on`: Standby writes WAL before acknowledgment (default for sync)
- `remote_write`: Standby acknowledges at OS level (lower durability)
- `remote_apply`: Standby applies transaction before acknowledgment (highest consistency)

**Configuration**:
```ini
# postgresql.conf on primary
synchronous_commit = on
synchronous_standby_names = 'ANY 1 (replica1, replica2)'
```

## Failover & Recovery

### Recovery Time Objective (RTO)

The elapsed, real-time duration for the data tier failover process to complete. RTO depends on the amount of time acceptable from a business perspective.

### Recovery Point Objective (RPO)

The amount of data loss (in elapsed real time) the data tier can sustain as a result of failover. RPO depends on the amount of data loss acceptable from a business perspective.

### Failover Process

The process of promoting a backup or standby infrastructure (replica node) to become the primary infrastructure. During failover, the replica node becomes the primary node.

### Switchover

A manual failover on a production system, either to test the system or to take the current primary node out of the cluster for maintenance.

### Fallback

The process of reinstating the former primary node after the condition that caused a failover is remedied.

## Backup & Recovery

### pg_dump / pg_restore

```bash
# Logical backup
pg_dump -h localhost -U postgres -d mydb -F c -f backup.dump

# Restore
pg_restore -h localhost -U postgres -d mydb -c backup.dump
```

### pg_basebackup

```bash
# Physical backup
pg_basebackup -h primary.example.com -D /var/lib/postgresql/backup -U replicator -v -P -W
```

### Continuous Archiving

```ini
# postgresql.conf
wal_level = replica
archive_mode = on
archive_command = 'cp %p /path/to/archive/%f'
```

### Point-in-Time Recovery (PITR)

```ini
# recovery.conf or postgresql.auto.conf
restore_command = 'cp /path/to/archive/%f %p'
recovery_target_time = '2024-01-01 12:00:00'
```

### Backup Tools

- **pgBackRest**: Enterprise-grade backup and restore tool
- **WAL-G**: Cloud-native backup tool supporting multiple cloud storage backends
- **pg_probackup**: PostgreSQL backup and recovery manager

## Monitoring

### Key Metrics

- **Replication Lag**: Monitor using `pg_stat_replication` view
- **Connection Count**: `pg_stat_activity`
- **Database Size**: `pg_database_size()`
- **Table Bloat**: `pg_stat_user_tables`
- **Index Usage**: `pg_stat_user_indexes`

### Monitoring Queries

```sql
-- Replication lag
SELECT 
    client_addr,
    state,
    sync_state,
    pg_wal_lsn_diff(pg_current_wal_lsn(), sent_lsn) AS sent_lag,
    pg_wal_lsn_diff(sent_lsn, write_lsn) AS write_lag,
    pg_wal_lsn_diff(write_lsn, flush_lsn) AS flush_lag,
    pg_wal_lsn_diff(flush_lsn, replay_lsn) AS replay_lag
FROM pg_stat_replication;

-- Active connections
SELECT count(*) FROM pg_stat_activity WHERE state = 'active';

-- Database sizes
SELECT 
    datname,
    pg_size_pretty(pg_database_size(datname)) AS size
FROM pg_database
ORDER BY pg_database_size(datname) DESC;
```

### Logging

```ini
# postgresql.conf
logging_collector = on
log_directory = 'log'
log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log'
log_statement = 'all'
log_duration = on
log_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h '
```

## Performance Optimization

### Query Optimization

Query optimization is crucial for PostgreSQL performance. Use the following techniques:

1. **Use EXPLAIN and EXPLAIN ANALYZE**: Analyze query execution plans
2. **Create appropriate indexes**: Based on query patterns
3. **Update statistics**: Run `ANALYZE` regularly
4. **Monitor slow queries**: Use `pg_stat_statements` extension
5. **Optimize joins**: Ensure proper indexes on join columns
6. **Use prepared statements**: For repeated queries

### EXPLAIN and Query Plans

**EXPLAIN:**
```sql
-- Basic explain
EXPLAIN SELECT * FROM users WHERE email = 'john@example.com';

-- Explain with costs
EXPLAIN (FORMAT TEXT) SELECT * FROM users WHERE email = 'john@example.com';

-- Explain with actual execution
EXPLAIN ANALYZE SELECT * FROM users WHERE email = 'john@example.com';

-- Explain with buffers
EXPLAIN (ANALYZE, BUFFERS) SELECT * FROM users WHERE email = 'john@example.com';

-- Explain in JSON format
EXPLAIN (FORMAT JSON) SELECT * FROM users WHERE email = 'john@example.com';
```

**Understanding Query Plans:**
- **Seq Scan**: Sequential scan (slow for large tables)
- **Index Scan**: Uses index to find rows
- **Index Only Scan**: Retrieves data from index only
- **Bitmap Heap Scan**: Uses bitmap index scan
- **Nested Loop**: Joins tables using nested loops
- **Hash Join**: Joins tables using hash table
- **Merge Join**: Joins sorted tables

**Query Plan Example:**
```sql
EXPLAIN ANALYZE SELECT u.username, o.total
FROM users u
INNER JOIN orders o ON u.id = o.user_id
WHERE u.created_at > '2024-01-01';
-- Output shows execution time, cost estimates, actual rows, index usage
```

### Indexing

**B-tree Index (Default):**
```sql
-- Create index
CREATE INDEX idx_users_email ON users(email);

-- Partial index (index subset of rows)
CREATE INDEX idx_active_users ON users(email) WHERE active = true;

-- Composite index (order matters!)
CREATE INDEX idx_users_name_email ON users(last_name, first_name, email);

-- Unique index
CREATE UNIQUE INDEX idx_users_username ON users(username);

-- Concurrent index creation (doesn't lock table)
CREATE INDEX CONCURRENTLY idx_users_email ON users(email);
```

**Index Types:**
- **B-tree**: Default, good for most queries
- **Hash**: Fast equality lookups (limited use cases)
- **GIN**: Generalized Inverted Index (JSONB, arrays, full-text)
- **GiST**: Generalized Search Tree (geometric, full-text)
- **SP-GiST**: Space-partitioned GiST
- **BRIN**: Block Range Index (large sorted tables)

**Index Best Practices:**
- Index columns used in WHERE, JOIN, and ORDER BY clauses
- Composite index column order matters (most selective first)
- Use partial indexes for filtered queries
- Monitor index usage with `pg_stat_user_indexes`
- Remove unused indexes (they slow down writes)

**Monitor Index Usage:**
```sql
-- View index usage statistics
SELECT 
    schemaname,
    tablename,
    indexname,
    idx_scan,
    idx_tup_read,
    idx_tup_fetch
FROM pg_stat_user_indexes
ORDER BY idx_scan DESC;

-- Find unused indexes
SELECT 
    schemaname,
    tablename,
    indexname
FROM pg_stat_user_indexes
WHERE idx_scan = 0
AND indexname NOT LIKE 'pg_toast%';
```

### pg_stat_statements

Enable query statistics tracking:

```sql
-- Enable extension
CREATE EXTENSION IF NOT EXISTS pg_stat_statements;

-- View top slow queries
SELECT 
    query,
    calls,
    total_exec_time,
    mean_exec_time,
    max_exec_time,
    stddev_exec_time
FROM pg_stat_statements
ORDER BY mean_exec_time DESC
LIMIT 10;

-- Reset statistics
SELECT pg_stat_statements_reset();
```

**Configuration (postgresql.conf):**
```ini
shared_preload_libraries = 'pg_stat_statements'
pg_stat_statements.track = all
pg_stat_statements.max = 10000
```

### Vacuum and Analyze

```sql
-- Manual vacuum
VACUUM ANALYZE;

-- Vacuum specific table
VACUUM ANALYZE users;

-- Aggressive vacuum (locks table)
VACUUM FULL users;

-- Vacuum with options
VACUUM VERBOSE ANALYZE users;

-- Auto-vacuum configuration
ALTER TABLE users SET (
    autovacuum_vacuum_scale_factor = 0.1,
    autovacuum_analyze_scale_factor = 0.05
);
```

**Vacuum Best Practices:**
- Run `VACUUM` regularly to reclaim space
- Use `VACUUM ANALYZE` to update statistics
- Avoid `VACUUM FULL` in production (locks table)
- Monitor `pg_stat_user_tables` for vacuum needs

## Stored Procedures and Functions

### PL/pgSQL

PL/pgSQL is PostgreSQL's procedural language for writing functions and stored procedures:

**Basic Function:**
```sql
CREATE OR REPLACE FUNCTION get_user_count()
RETURNS INTEGER AS $$
BEGIN
    RETURN (SELECT COUNT(*) FROM users);
END;
$$ LANGUAGE plpgsql;

-- Call function
SELECT get_user_count();
```

**Function with Parameters:**
```sql
CREATE OR REPLACE FUNCTION get_user_by_id(user_id INTEGER)
RETURNS TABLE(id INTEGER, username VARCHAR, email VARCHAR) AS $$
BEGIN
    RETURN QUERY
    SELECT users.id, users.username, users.email
    FROM users
    WHERE users.id = user_id;
END;
$$ LANGUAGE plpgsql;

-- Call function
SELECT * FROM get_user_by_id(1);
```

**Function with Variables:**
```sql
CREATE OR REPLACE FUNCTION calculate_order_total(order_id INTEGER)
RETURNS DECIMAL AS $$
DECLARE
    total_amount DECIMAL;
BEGIN
    SELECT SUM(price * quantity) INTO total_amount
    FROM order_items
    WHERE order_id = calculate_order_total.order_id;
    
    RETURN COALESCE(total_amount, 0);
END;
$$ LANGUAGE plpgsql;
```

### User-Defined Functions

**SQL Function:**
```sql
CREATE FUNCTION add_numbers(a INTEGER, b INTEGER)
RETURNS INTEGER AS $$
    SELECT a + b;
$$ LANGUAGE SQL;

SELECT add_numbers(5, 3);
```

**Function with Default Parameters:**
```sql
CREATE OR REPLACE FUNCTION get_users(
    limit_count INTEGER DEFAULT 10,
    offset_count INTEGER DEFAULT 0
)
RETURNS TABLE(id INTEGER, username VARCHAR) AS $$
BEGIN
    RETURN QUERY
    SELECT users.id, users.username
    FROM users
    ORDER BY users.id
    LIMIT limit_count
    OFFSET offset_count;
END;
$$ LANGUAGE plpgsql;
```

## Triggers

Triggers automatically execute functions when certain events occur:

**Create Trigger Function:**
```sql
CREATE OR REPLACE FUNCTION update_updated_at()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = CURRENT_TIMESTAMP;
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

-- Create trigger
CREATE TRIGGER update_users_updated_at
    BEFORE UPDATE ON users
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at();
```

**Audit Trigger:**
```sql
-- Create audit table
CREATE TABLE user_audit (
    id SERIAL PRIMARY KEY,
    user_id INTEGER,
    action VARCHAR(10),
    changed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    old_data JSONB,
    new_data JSONB
);

-- Create trigger function
CREATE OR REPLACE FUNCTION audit_user_changes()
RETURNS TRIGGER AS $$
BEGIN
    IF (TG_OP = 'DELETE') THEN
        INSERT INTO user_audit (user_id, action, old_data)
        VALUES (OLD.id, 'DELETE', row_to_json(OLD)::jsonb);
        RETURN OLD;
    ELSIF (TG_OP = 'UPDATE') THEN
        INSERT INTO user_audit (user_id, action, old_data, new_data)
        VALUES (NEW.id, 'UPDATE', row_to_json(OLD)::jsonb, row_to_json(NEW)::jsonb);
        RETURN NEW;
    ELSIF (TG_OP = 'INSERT') THEN
        INSERT INTO user_audit (user_id, action, new_data)
        VALUES (NEW.id, 'INSERT', row_to_json(NEW)::jsonb);
        RETURN NEW;
    END IF;
    RETURN NULL;
END;
$$ LANGUAGE plpgsql;

-- Create trigger
CREATE TRIGGER users_audit_trigger
    AFTER INSERT OR UPDATE OR DELETE ON users
    FOR EACH ROW
    EXECUTE FUNCTION audit_user_changes();
```

## Extensions

PostgreSQL supports extensions to add functionality:

**Common Extensions:**
```sql
-- Enable extension
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pg_trgm";  -- Trigram matching
CREATE EXTENSION IF NOT EXISTS "pg_stat_statements";  -- Query statistics
CREATE EXTENSION IF NOT EXISTS "btree_gin";  -- GIN index for btree
CREATE EXTENSION IF NOT EXISTS "btree_gist";  -- GiST index for btree

-- List installed extensions
SELECT * FROM pg_extension;

-- List available extensions
SELECT * FROM pg_available_extensions;
```

## Full-Text Search

PostgreSQL provides powerful full-text search capabilities:

**Create Full-Text Search Index:**
```sql
-- Add tsvector column
ALTER TABLE articles ADD COLUMN search_vector tsvector;

-- Create trigger to update search vector
CREATE OR REPLACE FUNCTION articles_search_update()
RETURNS TRIGGER AS $$
BEGIN
    NEW.search_vector := 
        setweight(to_tsvector('english', COALESCE(NEW.title, '')), 'A') ||
        setweight(to_tsvector('english', COALESCE(NEW.content, '')), 'B');
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER articles_search_trigger
    BEFORE INSERT OR UPDATE ON articles
    FOR EACH ROW
    EXECUTE FUNCTION articles_search_update();

-- Create GIN index
CREATE INDEX idx_articles_search ON articles USING GIN (search_vector);

-- Search
SELECT title, content
FROM articles
WHERE search_vector @@ to_tsquery('english', 'postgresql & database');

-- Ranked search
SELECT 
    title,
    ts_rank(search_vector, to_tsquery('english', 'postgresql')) AS rank
FROM articles
WHERE search_vector @@ to_tsquery('english', 'postgresql')
ORDER BY rank DESC;
```

## Partitioning

PostgreSQL supports table partitioning for large tables:

**Range Partitioning:**
```sql
-- Create partitioned table
CREATE TABLE orders (
    id SERIAL,
    user_id INTEGER,
    total DECIMAL(10, 2),
    created_at DATE
) PARTITION BY RANGE (created_at);

-- Create partitions
CREATE TABLE orders_2024_q1 PARTITION OF orders
    FOR VALUES FROM ('2024-01-01') TO ('2024-04-01');

CREATE TABLE orders_2024_q2 PARTITION OF orders
    FOR VALUES FROM ('2024-04-01') TO ('2024-07-01');

CREATE TABLE orders_2024_q3 PARTITION OF orders
    FOR VALUES FROM ('2024-07-01') TO ('2024-10-01');

CREATE TABLE orders_2024_q4 PARTITION OF orders
    FOR VALUES FROM ('2024-10-01') TO ('2025-01-01');
```

**List Partitioning:**
```sql
CREATE TABLE users (
    id SERIAL,
    username VARCHAR(50),
    region VARCHAR(50)
) PARTITION BY LIST (region);

CREATE TABLE users_north PARTITION OF users
    FOR VALUES IN ('NY', 'MA', 'CT');

CREATE TABLE users_south PARTITION OF users
    FOR VALUES IN ('TX', 'FL', 'GA');
```

**Hash Partitioning:**
```sql
CREATE TABLE orders (
    id SERIAL,
    user_id INTEGER,
    total DECIMAL(10, 2)
) PARTITION BY HASH (user_id);

CREATE TABLE orders_0 PARTITION OF orders
    FOR VALUES WITH (MODULUS 4, REMAINDER 0);

CREATE TABLE orders_1 PARTITION OF orders
    FOR VALUES WITH (MODULUS 4, REMAINDER 1);

CREATE TABLE orders_2 PARTITION OF orders
    FOR VALUES WITH (MODULUS 4, REMAINDER 2);

CREATE TABLE orders_3 PARTITION OF orders
    FOR VALUES WITH (MODULUS 4, REMAINDER 3);
```

## Infrastructure as Code (IaC)

Infrastructure as Code (IaC) allows you to manage PostgreSQL resources programmatically, enabling version control, automation, and consistent deployments. The [Pulumi PostgreSQL Provider](https://www.pulumi.com/registry/packages/postgresql/) provides a powerful way to manage PostgreSQL databases, users, roles, and other resources using code.

### Pulumi PostgreSQL Provider

Pulumi is an Infrastructure as Code platform that allows you to write, deploy, and manage infrastructure using familiar programming languages.

#### Installation

**JavaScript/TypeScript:**
```bash
npm install @pulumi/postgresql
```

**Python:**
```bash
pip install pulumi-postgresql
```

**Go:**
```bash
go get github.com/pulumi/pulumi-postgresql/sdk/v3/go/postgresql
```

**.NET:**
```bash
dotnet add package Pulumi.Postgresql
```

**Java:**
```xml
<dependency>
    <groupId>com.pulumi</groupId>
    <artifactId>postgresql</artifactId>
</dependency>
```

#### Provider Configuration

**Basic Configuration (Pulumi.yaml):**
```yaml
name: postgresql-config
runtime:
  name: nodejs
config:
  postgresql:connectTimeout:
    value: 15
  postgresql:database:
    value: postgres
  postgresql:host:
    value: postgres_server_ip
  postgresql:password:
    value: postgres_password
  postgresql:port:
    value: 5432
  postgresql:sslmode:
    value: require
  postgresql:username:
    value: postgres_user
```

**Environment Variables:**
```bash
export PGHOST=localhost
export PGPORT=5432
export PGUSER=postgres
export PGPASSWORD=postgres
export PGDATABASE=postgres
```

**SSL Client Certificate Configuration:**
```yaml
config:
  postgresql:database:
    value: postgres
  postgresql:host:
    value: postgres_server_ip
  postgresql:password:
    value: postgres_password
  postgresql:port:
    value: 5432
  postgresql:sslmode:
    value: require
  postgresql:username:
    value: postgres_user
  postgresql:clientcert:
    cert: /path/to/client.crt
    key: /path/to/client.key
    sslinline: false  # Set to true for inline cert/key
```

**SSL Root Certificate:**
```yaml
config:
  postgresql:sslrootcert:
    value: /path/to/ca.crt
```

#### Managing Resources

**TypeScript Example:**
```typescript
import * as pulumi from "@pulumi/pulumi";
import * as postgresql from "@pulumi/postgresql";

// Create database
const myDb = new postgresql.Database("my_db", {
    name: "my_database",
    encoding: "UTF8",
    lcCollate: "en_US.UTF-8",
    lcCtype: "en_US.UTF-8"
});

// Create role
const appRole = new postgresql.Role("app_role", {
    name: "app_user",
    login: true,
    password: "secure_password"
});

// Grant privileges
const grant = new postgresql.Grant("grant", {
    database: myDb.name,
    role: appRole.name,
    privileges: ["SELECT", "INSERT", "UPDATE", "DELETE"]
});

// Create schema
const appSchema = new postgresql.Schema("app_schema", {
    name: "app",
    owner: appRole.name
});
```

**Python Example:**
```python
import pulumi
import pulumi_postgresql as postgresql

# Create database
my_db = postgresql.Database("my_db",
    name="my_database",
    encoding="UTF8",
    lc_collate="en_US.UTF-8",
    lc_ctype="en_US.UTF-8"
)

# Create role
app_role = postgresql.Role("app_role",
    name="app_user",
    login=True,
    password="secure_password"
)

# Grant privileges
grant = postgresql.Grant("grant",
    database=my_db.name,
    role=app_role.name,
    privileges=["SELECT", "INSERT", "UPDATE", "DELETE"]
)
```

**Go Example:**
```go
package main

import (
    "github.com/pulumi/pulumi-postgresql/sdk/v3/go/postgresql"
    "github.com/pulumi/pulumi/sdk/v3/go/pulumi"
)

func main() {
    pulumi.Run(func(ctx *pulumi.Context) error {
        // Create database
        myDb, err := postgresql.NewDatabase(ctx, "my_db", &postgresql.DatabaseArgs{
            Name:     pulumi.String("my_database"),
            Encoding: pulumi.String("UTF8"),
        })
        if err != nil {
            return err
        }

        // Create role
        appRole, err := postgresql.NewRole(ctx, "app_role", &postgresql.RoleArgs{
            Name:     pulumi.String("app_user"),
            Login:    pulumi.Bool(true),
            Password: pulumi.String("secure_password"),
        })
        if err != nil {
            return err
        }

        return nil
    })
}
```

#### Authentication Methods

**Standard Authentication:**
```yaml
config:
  postgresql:host: value: postgres_server_ip
  postgresql:username: value: postgres_user
  postgresql:password: value: postgres_password
```

**AWS RDS IAM Authentication:**
```yaml
config:
  postgresql:awsRdsIamAuth:
    value: true
  postgresql:awsRdsEndpoint:
    value: mydb.123456789.us-east-1.rds.amazonaws.com
  postgresql:awsRdsRegion:
    value: us-east-1
```

**Azure Identity Authentication:**
```yaml
config:
  postgresql:azureIdentityAuth:
    value: true
  postgresql:azureTenantId:
    value: 'your-tenant-id'
  postgresql:host:
    value: 'your-server.fqdn'
  postgresql:username:
    value: 'Azure AD Admin Group'
```

**GCP Cloud SQL IAM Authentication:**
```yaml
config:
  postgresql:gcpCloudSqlIamAuth:
    value: true
  postgresql:host:
    value: '/cloudsql/project:region:instance'
```

#### Multi-Server Configuration

Configure multiple PostgreSQL servers using aliases:

**TypeScript:**
```typescript
import * as postgresql from "@pulumi/postgresql";

// Server 1
const db1 = new postgresql.Database("my_db1", {
    name: "my_db1"
}, { provider: postgresqlProvider1 });

// Server 2
const db2 = new postgresql.Database("my_db2", {
    name: "my_db2"
}, { provider: postgresqlProvider2 });
```

**Configuration Options:**
- `scheme`: Driver to use (`postgres`, `awspostgres`, `gcppostgres`)
- `host`: PostgreSQL server address
- `port`: Connection port (default: 5432)
- `database`: Database name (default: postgres)
- `username`: Connection username
- `password`: Connection password
- `sslmode`: SSL mode (`disable`, `require`, `verify-ca`, `verify-full`)
- `connectTimeout`: Maximum wait for connection in seconds (default: 180)
- `maxConnections`: Maximum open connections (default: 20)
- `superuser`: Set to false for non-superuser connections (e.g., AWS RDS, GCP SQL)
- `expectedVersion`: PostgreSQL version hint (default: 9.0.0)

**SOCKS5 Proxy Support:**
```bash
export ALL_PROXY=socks5://127.0.0.1:1080
export NO_PROXY=localhost,127.0.0.1
```

#### Common Use Cases

**Database and User Management:**
```typescript
// Create database with specific settings
const prodDb = new postgresql.Database("prod_db", {
    name: "production",
    encoding: "UTF8",
    lcCollate: "en_US.UTF-8",
    lcCtype: "en_US.UTF-8",
    template: "template0",
    allowConnections: true
});

// Create read-only user
const readOnlyUser = new postgresql.Role("readonly_user", {
    name: "readonly",
    login: true,
    password: "secure_password"
});

// Grant SELECT only
const readGrant = new postgresql.Grant("read_grant", {
    database: prodDb.name,
    role: readOnlyUser.name,
    schema: "public",
    privileges: ["SELECT"]
});
```

**Schema Management:**
```typescript
// Create schema
const appSchema = new postgresql.Schema("app_schema", {
    name: "application",
    owner: "app_user"
});

// Grant usage on schema
const schemaGrant = new postgresql.Grant("schema_grant", {
    database: prodDb.name,
    role: readOnlyUser.name,
    schema: appSchema.name,
    privileges: ["USAGE"]
});
```

**Table and Index Management:**
```typescript
// Note: Pulumi PostgreSQL provider focuses on database objects
// For table management, use SQL scripts or other tools
// The provider manages: databases, roles, schemas, grants, etc.
```

**Benefits of Using Pulumi:**
- **Version Control**: Infrastructure changes tracked in Git
- **Reproducibility**: Consistent deployments across environments
- **Automation**: Integrate with CI/CD pipelines
- **Multi-Language Support**: Use your preferred programming language
- **State Management**: Track resource state and changes
- **Preview Changes**: See what will change before applying

**Best Practices:**
1. **Use Secrets Management**: Store passwords in Pulumi secrets or external secret managers
2. **Environment Separation**: Use different stacks for dev/staging/prod
3. **Idempotency**: Ensure operations can be run multiple times safely
4. **Backup Before Changes**: Always backup before major infrastructure changes
5. **Review Changes**: Use `pulumi preview` before applying changes

## Security

### User and Role Management

PostgreSQL uses roles for authentication and authorization:

```sql
-- Create role
CREATE ROLE app_user WITH LOGIN PASSWORD 'secure_password';

-- Create user (alias for CREATE ROLE ... WITH LOGIN)
CREATE USER app_user WITH PASSWORD 'secure_password';

-- Grant privileges
GRANT SELECT, INSERT, UPDATE ON users TO app_user;
GRANT USAGE ON SCHEMA public TO app_user;

-- Revoke privileges
REVOKE DELETE ON users FROM app_user;

-- Alter role
ALTER ROLE app_user WITH CREATEDB;
ALTER ROLE app_user WITH SUPERUSER;

-- List roles
SELECT rolname, rolcanlogin FROM pg_roles;
```

### Privileges and Permissions

```sql
-- Grant table privileges
GRANT SELECT, INSERT, UPDATE, DELETE ON users TO app_user;

-- Grant column-level privileges
GRANT SELECT (id, username) ON users TO app_user;

-- Grant schema privileges
GRANT USAGE ON SCHEMA public TO app_user;
GRANT CREATE ON SCHEMA public TO app_user;

-- Grant database privileges
GRANT CONNECT ON DATABASE mydb TO app_user;
GRANT CREATE ON DATABASE mydb TO app_user;

-- View privileges
SELECT grantee, privilege_type 
FROM information_schema.role_table_grants 
WHERE table_name = 'users';
```

### Row-Level Security

Row-Level Security (RLS) allows fine-grained access control:

```sql
-- Enable RLS
ALTER TABLE orders ENABLE ROW LEVEL SECURITY;

-- Create policy
CREATE POLICY user_orders_policy ON orders
    FOR ALL
    TO app_user
    USING (user_id = current_setting('app.current_user_id')::INTEGER);

-- Policy for SELECT
CREATE POLICY select_own_orders ON orders
    FOR SELECT
    TO app_user
    USING (user_id = current_setting('app.current_user_id')::INTEGER);

-- Policy for INSERT
CREATE POLICY insert_own_orders ON orders
    FOR INSERT
    TO app_user
    WITH CHECK (user_id = current_setting('app.current_user_id')::INTEGER);

-- View policies
SELECT * FROM pg_policies WHERE tablename = 'orders';
```

### Encryption

**Encryption at Rest:**
- Use filesystem-level encryption (LUKS, etc.)
- Use Transparent Data Encryption (TDE) if available

**Encryption in Transit:**
```ini
# postgresql.conf
ssl = on
ssl_cert_file = '/etc/ssl/certs/server.crt'
ssl_key_file = '/etc/ssl/private/server.key'
```

**Encrypting Data:**
```sql
-- Enable pgcrypto extension
CREATE EXTENSION IF NOT EXISTS pgcrypto;

-- Encrypt data
INSERT INTO users (username, password_hash)
VALUES ('john', crypt('password123', gen_salt('bf')));

-- Verify password
SELECT * FROM users 
WHERE username = 'john' 
AND password_hash = crypt('password123', password_hash);
```

### SSL/TLS Configuration

```ini
# postgresql.conf
ssl = on
ssl_cert_file = '/etc/ssl/certs/server.crt'
ssl_key_file = '/etc/ssl/private/server.key'
ssl_ca_file = '/etc/ssl/certs/ca.crt'
ssl_ciphers = 'HIGH:MEDIUM:+3DES:!aNULL'
```

```bash
# pg_hba.conf
hostssl    all    all    0.0.0.0/0    md5
```

## Import & Export Data

### COPY Command

The COPY command is the fastest way to import/export data in PostgreSQL:

**Export to CSV:**
```sql
-- Export table to CSV
COPY users TO '/tmp/users.csv' WITH CSV HEADER;

-- Export query results
COPY (SELECT username, email FROM users WHERE status = 'active') 
TO '/tmp/active_users.csv' WITH CSV HEADER;

-- Export with delimiter
COPY users TO '/tmp/users.txt' WITH DELIMITER '|';
```

**Import from CSV:**
```sql
-- Import from CSV
COPY users FROM '/tmp/users.csv' WITH CSV HEADER;

-- Import specific columns
COPY users (username, email) FROM '/tmp/users.csv' WITH CSV HEADER;

-- Import with delimiter
COPY users FROM '/tmp/users.txt' WITH DELIMITER '|';
```

**Using psql \copy:**
```bash
# Export (client-side, doesn't require superuser)
psql -d mydb -c "\copy users TO '/tmp/users.csv' WITH CSV HEADER"

# Import (client-side)
psql -d mydb -c "\copy users FROM '/tmp/users.csv' WITH CSV HEADER"
```

**COPY vs \copy:**
- **COPY**: Server-side, requires superuser or file on server
- **\copy**: Client-side, works with local files, no superuser required

## psql Commands

psql is PostgreSQL's interactive command-line tool:

**Connection:**
```bash
# Connect to database
psql -d mydb -U username

# Connect with password prompt
psql -d mydb -U username -W

# Connect with connection string
psql postgresql://username:password@localhost:5432/mydb
```

**Common psql Commands:**
```sql
-- List databases
\l

-- Connect to database
\c mydb

-- List tables
\dt

-- List tables in schema
\dt schema_name.*

-- Describe table structure
\d table_name

-- Describe table with details
\d+ table_name

-- List indexes
\di

-- List views
\dv

-- List functions
\df

-- List schemas
\dn

-- List users/roles
\du

-- Show current database
SELECT current_database();

-- Show current user
SELECT current_user;

-- Show version
SELECT version();

-- Show all settings
SHOW ALL;

-- Show specific setting
SHOW shared_buffers;

-- Execute SQL file
\i /path/to/script.sql

-- Export query results to file
\o /tmp/results.txt
SELECT * FROM users;
\o

-- Timing queries
\timing

-- Toggle expanded display
\x

-- Show query execution plan
EXPLAIN SELECT * FROM users;

-- Exit psql
\q
```

**psql Configuration (.psqlrc):**
```bash
# ~/.psqlrc
\set PROMPT1 '%[%033[1;33;40m%]%n@%/%R%[%033[0m%]%# '
\set PROMPT2 '%[%033[1;36;40m%]%R%[%033[0m%]%# '
\timing
```

## PostgreSQL Recipes

### Compare Two Tables

```sql
-- Find rows in table1 but not in table2
SELECT * FROM table1
EXCEPT
SELECT * FROM table2;

-- Find differences using FULL OUTER JOIN
SELECT 
    COALESCE(t1.id, t2.id) AS id,
    t1.column1 AS table1_value,
    t2.column1 AS table2_value
FROM table1 t1
FULL OUTER JOIN table2 t2 ON t1.id = t2.id
WHERE t1.column1 IS DISTINCT FROM t2.column1;
```

### Delete Duplicate Rows

```sql
-- Using CTE and ROW_NUMBER
WITH duplicates AS (
    SELECT id,
           ROW_NUMBER() OVER (PARTITION BY email ORDER BY id) AS rn
    FROM users
)
DELETE FROM users
WHERE id IN (
    SELECT id FROM duplicates WHERE rn > 1
);

-- Keep the row with the lowest id
DELETE FROM users u1
USING users u2
WHERE u1.email = u2.email
AND u1.id > u2.id;
```

### Generate Random Number in Range

```sql
-- Random integer between 1 and 100
SELECT floor(random() * 100 + 1)::INTEGER;

-- Random number between min and max
SELECT floor(random() * (max - min + 1) + min)::INTEGER;

-- Random timestamp in range
SELECT 
    '2024-01-01'::timestamp + 
    random() * ('2024-12-31'::timestamp - '2024-01-01'::timestamp);
```

### Temporary Tables

```sql
-- Create temporary table
CREATE TEMP TABLE temp_users AS
SELECT * FROM users WHERE created_at > '2024-01-01';

-- Temporary table (session-scoped)
CREATE TEMPORARY TABLE session_data (
    id SERIAL PRIMARY KEY,
    data TEXT
);

-- Global temporary table (shared across sessions)
CREATE TEMP TABLE global_temp_data (
    id SERIAL PRIMARY KEY,
    data TEXT
) ON COMMIT PRESERVE ROWS;  -- or ON COMMIT DELETE ROWS
```

### Copy Table

```sql
-- Copy table structure only
CREATE TABLE users_copy (LIKE users INCLUDING ALL);

-- Copy table with data
CREATE TABLE users_copy AS SELECT * FROM users;

-- Copy with WHERE clause
CREATE TABLE active_users_copy AS 
SELECT * FROM users WHERE status = 'active';
```

## Connection Pooling

Connection pooling helps efficiently manage database connections, reducing overhead and improving performance.

### PgBouncer

[PgBouncer](https://www.pgbouncer.org/) is a lightweight connection pooler for PostgreSQL.

**Configuration (pgbouncer.ini)**:
```ini
[databases]
mydb = host=localhost port=5432 dbname=mydb

[pgbouncer]
listen_addr = *
listen_port = 6432
auth_type = md5
auth_file = /etc/pgbouncer/userlist.txt
pool_mode = transaction
max_client_conn = 1000
default_pool_size = 25
```

**Pool Modes:**
- **session**: One server connection per client (default)
- **transaction**: One server connection per transaction
- **statement**: One server connection per statement

### Pgpool-II

[Pgpool-II](https://www.pgpool.net/) provides connection pooling, load balancing, and replication management.

## Best Practices

### Maintenance and Monitoring

1. **Regular Backups**: Back up databases regularly and test recovery process
2. **Monitor Replication Lag**: Set up alerts for significant lag increases
3. **Connection Pooling**: Use PgBouncer or Pgpool-II for connection management
4. **Comprehensive Monitoring**: Monitor CPU, memory, disk I/O, network, and active connections
5. **Log Collection**: Collect server logs, WAL logs, and autovacuum logs
6. **Alerting**: Integrate metrics and logs with alerting systems

### High Availability

1. **Test Failover**: Regularly test failover processes
2. **Monitor Health**: Continuously monitor PostgreSQL and system health
3. **Automate Recovery**: Implement automation for failure detection and recovery
4. **Document Procedures**: Document failover and recovery procedures
5. **Plan for Fallback**: Ensure fallback procedures are tested and documented

### Performance

1. **Tune Configuration**: Adjust `shared_buffers`, `effective_cache_size`, and `work_mem` based on workload
2. **Index Strategically**: Create indexes on frequently queried columns
3. **Monitor Query Performance**: Use `pg_stat_statements` to identify slow queries
4. **Regular Maintenance**: Run `VACUUM` and `ANALYZE` regularly
5. **Connection Management**: Use connection pooling to manage connections efficiently

## Resources

### Official Documentation

The [PostgreSQL Official Documentation](https://www.postgresql.org/docs/) is comprehensive and covers all aspects of PostgreSQL:

**Core Documentation:**
- [PostgreSQL Documentation](https://www.postgresql.org/docs/) - Main documentation index
- [SQL Language Reference](https://www.postgresql.org/docs/current/sql.html) - SQL command reference
- [Data Types](https://www.postgresql.org/docs/current/datatype.html) - Complete data type reference
- [Functions and Operators](https://www.postgresql.org/docs/current/functions.html) - Built-in functions
- [Indexes](https://www.postgresql.org/docs/current/indexes.html) - Index types and usage
- [Performance Tips](https://www.postgresql.org/docs/current/performance-tips.html) - Performance optimization guide
- [High Availability, Load Balancing, and Replication](https://www.postgresql.org/docs/current/high-availability.html) - HA and replication guide

**Administration:**
- [Server Configuration](https://www.postgresql.org/docs/current/runtime-config.html) - Configuration parameters
- [Database Roles and Privileges](https://www.postgresql.org/docs/current/user-manag.html) - User and role management
- [Backup and Restore](https://www.postgresql.org/docs/current/backup.html) - Backup strategies
- [Routine Database Maintenance Tasks](https://www.postgresql.org/docs/current/maintenance.html) - Maintenance procedures

**Advanced Topics:**
- [PL/pgSQL - SQL Procedural Language](https://www.postgresql.org/docs/current/plpgsql.html) - Stored procedures
- [Triggers](https://www.postgresql.org/docs/current/triggers.html) - Database triggers
- [Full Text Search](https://www.postgresql.org/docs/current/textsearch.html) - Full-text search
- [Partitioning](https://www.postgresql.org/docs/current/ddl-partitioning.html) - Table partitioning
- [Row Security Policies](https://www.postgresql.org/docs/current/ddl-rowsecurity.html) - Row-level security

### High Availability Solutions

- [Patroni Documentation](https://patroni.readthedocs.io/)
- [pg_auto_failover Documentation](https://pg-auto-failover.readthedocs.io/)
- [Architectures for high availability of PostgreSQL clusters on Compute Engine](https://docs.cloud.google.com/architecture/architectures-high-availability-postgresql-clusters-compute-engine)

### Tools and Extensions

- [pgBackRest](https://github.com/pgbackrest/pgbackrest) - Backup and restore tool
- [WAL-G](https://github.com/wal-g/wal-g) - Cloud-native backup tool
- [PgBouncer](https://www.pgbouncer.org/) - Connection pooler
- [Pgpool-II](https://www.pgpool.net/) - Connection pooling and load balancing

### Infrastructure as Code

- [Pulumi PostgreSQL Provider](https://www.pulumi.com/registry/packages/postgresql/) - Infrastructure as Code for PostgreSQL using Pulumi
- [Terraform PostgreSQL Provider](https://registry.terraform.io/providers/cyrilgdn/postgresql/latest/docs) - Infrastructure as Code for PostgreSQL using Terraform

### Tutorials and Learning Resources

- [PostgreSQL Tutorial (PostgreSQLTutorial.com)](https://neon.com/postgresql/tutorial) - Comprehensive PostgreSQL tutorial with practical examples
- [PostgreSQL Wiki](https://wiki.postgresql.org/) - Community wiki with additional resources and guides
- [PostgreSQL Mailing Lists](https://www.postgresql.org/list/) - Official mailing lists for support and discussions
- [PostgreSQL IRC Channel](https://www.postgresql.org/community/irc/) - Real-time community support

---

## References

1. [PostgreSQL Official Documentation](https://www.postgresql.org/docs/) - Comprehensive PostgreSQL documentation covering all features, SQL syntax, administration, and advanced topics
2. [PostgreSQL Tutorial (PostgreSQLTutorial.com)](https://neon.com/postgresql/tutorial) - Practical PostgreSQL tutorial with examples covering SQL fundamentals, data types, advanced queries, and administration
3. [Pulumi PostgreSQL Provider](https://www.pulumi.com/registry/packages/postgresql/) - Infrastructure as Code provider for managing PostgreSQL resources programmatically
4. [Architectures for high availability of PostgreSQL clusters on Compute Engine - Google Cloud](https://docs.cloud.google.com/architecture/architectures-high-availability-postgresql-clusters-compute-engine) - HA architecture patterns and best practices
5. [PostgreSQL High-Availability Cluster (gecio/postgresql_cluster) - GitHub](https://github.com/gecio/postgresql_cluster) - Ansible playbook for deploying PostgreSQL HA clusters with Patroni
6. [Patroni - High Availability Template for PostgreSQL](https://github.com/zalando/patroni) - Mature HA solution for PostgreSQL
7. [pg_auto_failover - Automated PostgreSQL Failover](https://github.com/citusdata/pg_auto_failover) - Automated failover solution for PostgreSQL
8. [PostgreSQL Wiki](https://wiki.postgresql.org/) - Community wiki with additional resources and guides
9. [PostgreSQL Performance Tuning](https://www.postgresql.org/docs/current/performance-tips.html) - Official performance optimization guide
10. [PostgreSQL High Availability Documentation](https://www.postgresql.org/docs/current/high-availability.html) - Official HA and replication documentation

